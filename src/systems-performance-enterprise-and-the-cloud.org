#+title: 系统性能：企业与云计算(Systems Performance: Enterprise and the Cloud)

http://www.brendangregg.com/blog/2020-07-15/systems-performance-2nd-edition.html

https://book.douban.com/subject/26586598/

这本书主要讲的是系统性能分析方法和优化手段，结合的环境是企业部署和云计算，即大规模系统。我印象比较深刻的就是，如何展示出一个集群下面上千个多核CPU的使用状况。

云计算是未来的趋势，这一点对于技术人员在思考问题和解决问题时非常重要：对于应用开发人员来说，我是否可以利用云来节省成本；对于云环境开发人员来说，我做的事情是否可以扩展到云环境下。

虽然这本书介绍了不少优化工具和手段，但是实际情况是复杂的，可能书中介绍到的工具或者是手段根本就用不上，所以专注在方法和原理会更加重要。从方法和原理出发，结合实际情况，找到可以使用的工具和手段，才是正确的路径。

** 第2章 方法

系统的各种延时

| 事件               | 延时        | 相对时间比例 |
|--------------------+-------------+--------------|
| 1个CPU周期         | 0.3ns(3GHz) | 1s           |
| L1缓存             | 0.9ns       | 3s           |
| L2缓存             | 2.8ns       | 9s           |
| L3缓存             | 12.9ns      | 43s          |
| 内存访问           | 120ns       | 6分          |
| SSD                | 50-150us    | 2-6天        |
| 旋转磁盘           | 1-10ms      | 1-12月       |
| 互联网：CA-NY      | 40ms        | 4年          |
| 互联网：CA-UK      | 81ms        | 8年          |
| 互联网：CA-AU      | 183ms       | 19年         |
| TCP包重传          | 1-3s        | 105-317年    |
| OS虚拟化系统重启   | 4s          | 423年        |
| SCSI命令超时       | 30s         | 3千年        |
| 硬件虚拟化系统重启 | 40s         | 4千年        |
| 物理系统重启       | 5m          | 32千年       |

----------

术语“使用率”经常用于描述设备的使用情况，诸如CPU和磁盘设备，有基于时间定义，也有基于容量定义的。基于容量的定义着眼于资源本身可以完成多少事情，那么100%使用率的CPU或者是磁盘是没有办法完成更多任务的；而基于时间的定义着眼于资源有多少比例的时间在处于使用状态（或者是处于空闲这状态），那么100%使用率的CPU或者是磁盘可以通过排队来完成更多的任务。

在大多数情况下，我们更喜欢使用基于时间定义的“使用率”，比如CPU或者是磁盘，如果他们正在满负荷运行的话，那么新来的任务就只能排队。这个排队数量也常被称为饱和度。但是也有一些资源是基于容量的，比如内存，这个就没有饱和度可言。

USE方法(utilization, saturation, errors)应用于性能研究，用来识别系统瓶颈：对于所有的资源，查看它的使用率，饱和度和错误。

----------

建立系统的分析模型有很多用途，特别是对于可扩展性分析：研究当负载或资源扩展时性能会如何变化，这里的资源可以是硬件如CPU核，也可以是软件如进程或线程。

除对生产系统的观测（测量）和实验性测试（仿真）之外，分析建模可以被认为是第三类性能评估方法。上述三者至少择其二可让性能研究最为透彻：分析建模和仿真，或仿真和测量。

如果是对一个现有系统做分析，可以从测量开始，归纳负载特征和测量性能。如果系统没有生产环境复杂或要测试的工作负载在生产环境不可见，可以用工作负载仿真做测试，建模基于测试和仿真的结果，用于性能预测。

可扩展性分析可以揭示性能由于资源限制停止线性增长的点，即拐点。找到这些点是否存在或存在在哪里，这对研究阻碍系统扩展性的性能问题有指导意义。

----------

下面是一系列性能扩展性曲线，没有严格的模型，但用视觉可以识别出各种类型。对于每一条曲线，x轴是扩展的维度，y轴是相应的性能（吞吐量，每秒事务速等等），曲线的类型如下：
- 线性扩展：性能随着资源的扩展成比例的增加，这种情况并非永久持续，但这可能是其他扩展情况的早期阶段。
- 竞争：架构的某些组件是共享的，而且只能串行使用，对这些共享资源的竞争会减少扩展的效益。（在拐点处性能放缓）
- 一致性：由于要维持数据的一致性，传播数据变化的代价会超过扩展带来的好处。（超过拐点性能下降）
- 拐点：某个因素碰到了扩展的制约点，从而改变了扩展曲线。
- 扩展上限：到达了一个性能的极限。该极限可能是设备瓶颈，诸如总线或互连器件到了吞吐量的最大值，或者是一个软件设置的限制（系统资源控制）。

----------

排队理论 Kendall标记法

该标记发为每一个属性指定一个符号，格式如下：A/S/m. 到达过程A, 服务时间分布S以及服务中心数目m。标记法还有扩展格式以及囊括更多的要素：系统中缓冲数目，任务数目上限和服务规则。通常研究的排队系统如下：
- M/M/1: 马尔可夫到达（指数分布到达），马尔可夫服务时间（指数分布），一个服务中心。
- M/M/c: 和M/M/1一样，但是服务中心有多个。
- M/G/1: 马尔可夫到达，服务时间是一般分布，一个服务中心。（通常用于研究旋转的物理硬盘性能）
- M/D/1: 马尔科夫到达，确定性的服务时间（固定时间），一个服务中心。

其中M/D/1是一个相对简单的示例。这个模型有个特点就是：如果使用率超过60%，那么平均响应时间会成为2倍；如果使用率超过80%，那么时间会成为3倍。

----------

可视化 线图/散点图/热图

线图相对简单，为了观察到长尾，最好设置几个百分位：50%，90%，99%.

散点图相比线图，可以观察到具体点的分布情况。在点数量少的情况下没有问题，点重合严重的时候就难以辨认。

热图通过把x和y轴的区域分组量化，能够解决散点图的扩展问题，所分成的组成为“桶”。这些“桶”是涂色的，颜色是依据落在x和y轴区域内的事件数目而定。这样量化既解决了散点图可视化密度上的限制，又使得热图不管显示的是单个系统还是成千上万个系统，都可以用一样的方法。（扩展性很好）

** 第4章 观测工具

性能观测工具可以按照两个维度划分：范围（系统和进程），方式（计数和跟踪）

| 范围\方式 | 计数                     | 跟踪                          |
|-----------+--------------------------+-------------------------------|
| 系统      | vmstat/mpstat/iostat/sar | tcpdump/dtrace/systemtap/perf |
| 进程      | ps/top/pmap              | strace/gdb/truss/mdb          |

----------

静态和动态追踪

静态追踪是通过在编译之前在源代码中增加静态探针实现的。在源代码中没有可见的动态探针的例子，因为动态探针的是在编译后软件运行时加入的。

动态探针会在函数的入口，利用内核空间的现场修改功能(live patching)，将第1个指令变为int的软中断指令，而该软中断已经接到指示执行跟踪的action。内核空间的现场修改功(live patching)能所采用的技术，会因处理器类型的不同而有所不同。

当动态跟踪被开启时，指令才会被替换；而动态跟踪被禁用时，指令会回到原来的状态。所以只有当开启的时候才会存在开销，并且这个开销是和执行action的频率成比例的。


----------

SystemTap 采用其他的内核框架作源：静态探针用tracepoints，动态探针用kprobes，用户级别的探针用uprobes。这些源以为其他的工具所用（perf, LTTng）。

Linux性能事件（Linux Performance Events, LPE），简称为perf，通过不断演化，现在所支持的性能观测的范围已经相当宽泛。虽然没有DTtrace和SystemTap那样的实时编程能力，但perf可以执行静态和动态追踪（基于tracepoints, kprobe和uprobe），还有profiling。此外它还能检查跟踪，局部变量和数据类型。因为它已经成为Linux内核主线的一部分，所以是最容易使用的，所提供的观测能力足够解答你的疑问。
