#+title: SRE Google运维解密(SRE: How Google Runs Production Systems)

https://book.douban.com/subject/26875239/

** 第2章 Google生产环境

下图描绘了一个典型的Google数据中心的拓扑结构。
1. 约10台物理服务器组成了一个机柜(rack)
2. 数台机柜组成一个机柜台(row)
3. 一排或多排机柜组成一个集群(cluster)
4. 一般来说，一个数据中心(datacenter)包含多个集群
5. 多个相邻的数据中心组成了一个园区(campus)

file:images/google-campus.png


每个数据中心内的物理服务器都需要能够互相进行网络通信。为了解决这个问题，我们用几百台Google自己制造的交换机以clos连接方式连接起来，创建了一个非常快的虚拟网络交换机，这个交换机有几万个虚拟端口。这个交换机的名字叫Jupiter。在Google最大的一个数据中心内，Jupiter可以提供1.3PB/s的交叉带宽。

Google的数据中心是由一套全球覆盖的骨干网B4连接起来的，B4是基于SDN网络技术（使用OpenFlow标准协议）构建的，可以给中型规模的骨干网络提供海量带宽，同时可以利用动态带宽管理优化网络连接，最大化平均带宽。

** 第7章 Google的自动化系统的演进

可靠性是最基本的功能

当然为了有效的进行故障调试，自我检查中所依赖的内部运作细节也应该暴露给管理整体系统的操作员。在非计算机领域对自动化影响的类似讨论，例如民航或工业应用中，经常会指出高效的自动化的缺点。随着时间的推移，操作员与系统的有用的直接接触会减少，因为自动化会覆盖越来越多的日常活动。不可避免的，当自动化系统出现问题时，操作员将无法成功的操作该系统。

由于缺乏实践，他们已经丧失了反应的流畅性，他们有关系统“应该”做什么的心理模型不再反映现实中系统“正在进行”的活动。这种情况在系统非自主运行时出现的更多，即，当自动化逐渐取代了手动操作，假设其他的手工操作仍然可能执行，并且如之前一样一直可用。令人难过的事，随着时间的推移，这个假设终将不再正确：这些手动操作最后将无法执行，因为允许它们执行的功能已经不存在。

Google也经历过自动化在某些条件下是有害的情况。参看下面“自动化允许大规模故障发生”的补充材料。但是以Google的经验来看，在更多的系统中自动化和自主化的行为不再是可选的附加项。随着服务规模扩大，肯定是这样的，但是不论系统规模大小，系统中具有更多自主行为的系统，仍然有很多好处。可靠性是最基本的功能，并且自主性和弹性行为是达到这一特征的有效途径。

** 第21章 应对过载

QPS陷阱

不同的请求可能需要数量迥异的资源来处理。某个请求的成本可能由各种各样的因素决定，例如客户端的代码不同（有的服务有很多种客户端实现），或者甚至是当时的现实时间（家庭用户和企业用户，交互请求和批量请求）。

如果在多年的经验积累中得出：按照QPS来规划服务容量，或者是按照某种静态属性（认为其能指代处理所消耗的资源:例如某个请求所需要读取key-value数量）一般是错误的选择。就算这个指标在某一个时间段内看起来工作还算良好，早晚也会发生变化。有些变动是逐渐发生的，有些则是非常突然的（例如某个软件的新版本突然使得某些请求消耗的资源大幅减少）。这种不断变动的目标，使得设计和实现良好的负载均衡策略使用起来非常困难。

更好的解决方案是直接用以可用资源来衡量可用容量。例如某个服务可能在某个数据中心预留了500CPU内核和1TB内存用以提供服务，用这些数字来建模该数据中心的服务容量是非常合理的。我们经常将某个请求的“成本”定义为，该请求在正常情况下所消耗的CPU时间（这里要考虑到不同的CPU类型的性能差异问题）。

在绝大部分情况下，我们发现简单的使用CPU数量作为资源配给的主要信号就可以工作得很好，原因如下：
1. 在有垃圾回收GC机制的编程环境，内存的压力通常自然而然地变成CPU的压力。在内存受限的情况下，GC会增加。
2. 在其他编程环境里，其他资源一般可通过某种比例进行配置，以便这些资源的短缺情况非常罕见。
如果过量分配其他非CPU资源不可行的话，我们可以在计算资源消耗的时候，将各种系统资源分别考虑在内。
