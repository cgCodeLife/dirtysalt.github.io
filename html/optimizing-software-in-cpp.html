<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Optimizing software in C++(An optimization guide for Windows, Linux and Mac platforms)</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="dirtysalt" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content">
<h1 class="title">Optimizing software in C++(An optimization guide for Windows, Linux and Mac platforms)</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. 1. Introduction</a></li>
<li><a href="#sec-2">2. 2. Choosing the optimal platform</a></li>
<li><a href="#sec-3">3. 3. Finding the biggest time consumers</a></li>
<li><a href="#sec-4">4. 4. Performance and usability</a></li>
<li><a href="#sec-5">5. 5. Choosing the optimal algorithm</a></li>
<li><a href="#sec-6">6. 6. Development process</a></li>
<li><a href="#sec-7">7. The efficiency of different C++ constructs</a></li>
<li><a href="#sec-8">8. Optimizations in the compiler</a>
<ul>
<li><a href="#sec-8-1">8.1. How compilers optimize</a></li>
<li><a href="#sec-8-2">8.2. Comparison of different compilers</a></li>
<li><a href="#sec-8-3">8.3. Obstacles to optimization by compiler</a></li>
<li><a href="#sec-8-4">8.4. Obstacles to optimization by CPU</a></li>
<li><a href="#sec-8-5">8.5. Compiler optimization options</a></li>
<li><a href="#sec-8-6">8.6. Optimization directives</a></li>
<li><a href="#sec-8-7">8.7. Checking what the compiler does</a></li>
</ul>
</li>
<li><a href="#sec-9">9. Optimizing memory access</a>
<ul>
<li><a href="#sec-9-1">9.1. Caching of code and data</a></li>
<li><a href="#sec-9-2">9.2. Cache organization</a></li>
<li><a href="#sec-9-3">9.3. Functions that are used together should be stored together</a></li>
<li><a href="#sec-9-4">9.4. Variables that are used together should be stored together</a></li>
<li><a href="#sec-9-5">9.5. Alignment of data</a></li>
<li><a href="#sec-9-6">9.6. Dynamic memory allocation</a></li>
<li><a href="#sec-9-7">9.7. Container classes</a></li>
<li><a href="#sec-9-8">9.8. Strings</a></li>
<li><a href="#sec-9-9">9.9. Access data sequentially</a></li>
<li><a href="#sec-9-10">9.10. Cache contentions in large data structures</a></li>
<li><a href="#sec-9-11">9.11. Explicit cache control</a></li>
</ul>
</li>
<li><a href="#sec-10">10. Multithreading</a>
<ul>
<li><a href="#sec-10-1">10.1. Hyperthreading</a></li>
</ul>
</li>
<li><a href="#sec-11">11. Out of order execution</a></li>
<li><a href="#sec-12">12. Using vector operations</a></li>
<li><a href="#sec-13">13. Making critical code in multiple versions for different instruction sets</a></li>
<li><a href="#sec-14">14. Specific optimization topics</a>
<ul>
<li><a href="#sec-14-1">14.1. Use lookup tables</a></li>
<li><a href="#sec-14-2">14.2. Bounds checking</a></li>
<li><a href="#sec-14-3">14.3. Use bitwise operators for checking multiple values at once</a></li>
<li><a href="#sec-14-4">14.4. Integer multiplication</a></li>
<li><a href="#sec-14-5">14.5. Integer division</a></li>
<li><a href="#sec-14-6">14.6. Floating point division</a></li>
<li><a href="#sec-14-7">14.7. Don't mix float and double</a></li>
<li><a href="#sec-14-8">14.8. Conversions between floating point numbers and integers</a></li>
<li><a href="#sec-14-9">14.9. Using integer operations for manipulating floating point variables</a></li>
<li><a href="#sec-14-10">14.10. Mathematical functions</a></li>
<li><a href="#sec-14-11">14.11. Static versus dynamic libraries</a></li>
<li><a href="#sec-14-12">14.12. Position-independent code</a></li>
<li><a href="#sec-14-13">14.13. System programming</a></li>
</ul>
</li>
<li><a href="#sec-15">15. Metaprogramming</a></li>
<li><a href="#sec-16">16. Testing speed</a>
<ul>
<li><a href="#sec-16-1">16.1. The pitfalls of unit-testing</a></li>
<li><a href="#sec-16-2">16.2. Worst-case testing</a></li>
</ul>
</li>
<li><a href="#sec-17">17. Optimization in embedded systems</a></li>
<li><a href="#sec-18">18. Overview of compiler options</a></li>
</ul>
</div>
</div>
<p>
<a href="http://www.agner.org/optimize/">http://www.agner.org/optimize/</a>
</p>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> 1. Introduction</h2>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> 2. Choosing the optimal platform</h2>
<div class="outline-text-2" id="text-2">
<p>
主处理器搭配上为特殊任务定制的协处理器可以有效地提高性能，而FPGA将有效地降低了制作协处理器的成本。
</p>

<p>
It is possible to implement a microprocessor in an FPGA as a so-called soft processor. Such a soft processor is much slower than a dedicated microprocessor and therefore not advantageous by itself. But a solution where a soft processor activates critical application- specific instructions that are coded in a hardware definition language in the same chip can be a very efficient solution in some cases. An even more powerful solution is the combination of a dedicated microprocessor core and an FPGA in the same chip. Such hybrid solutions are now used in some embedded systems.
</p>

<p>
64位处理器相比32位在CPU密集型任务上可以有5-10%的性能提升，并且支持更大内存，而其他情况则没有太大差异，真是免费的午餐。
</p>

<p>
64-bit operating systems are common today. These system are capable of running both 32- bit and 64-bit programs. The 64-bit systems can improve the performance by 5-10% for some CPU-intensive applications with many function calls and for applications that use large amounts of RAM memory. If the bottleneck is elsewhere, then there is no difference in performance between 32-bit and 64-bit systems. Applications that use large amounts of memory will benefit from the larger address space of the 64-bit systems.
</p>

<p>
MacOSX上默认使用PIC方式来编译，PIC在函数初次调用上的时候有符号解析的时间，如果想继续提升效率则可以考虑静态编译（PIC代码是为编译出动态库设计的）
</p>

<p>
The Intel-based Mac OS X operating system is based on BSD, but the compiler uses position-independent code and lazy binding by default, which makes it less efficient. The performance can be improved by using static linking and by not using position-independent code (option -fno-pic).
</p>


<p>
下表是64位和32位系统的对比。粗看下来就是64位系统都是优势，更多的寄存器，支持更大的内存，支持更大的寻址空间，缺点就是指令上稍微大些不太利于缓存。
</p>

<p>
64 bit systems have several advantages over 32 bit systems:
</p>
<ul class="org-ul">
<li>The number of registers is doubled. This makes it possible to store intermediate data and local variables in registers rather than in memory.
</li>
<li>Function parameters are transferred in registers rather than on the stack. This makes function calls more efficient.
</li>
<li>The size of the integer registers is extended to 64 bits. This is only an advantage in applications that can take advantage of 64-bit integers.
</li>
<li>The allocation and deallocation of big memory blocks is more efficient.
</li>
<li>The SSE2 instruction set is supported on all 64-bit CPUs and operating systems.
</li>
<li>The 64 bit instruction set supports self-relative addressing of data. This makes position-independent code more efficient.
</li>
</ul>
<p>
64 bit systems have the following disadvantages compared to 32 bit systems:
</p>
<ul class="org-ul">
<li>Pointers, references, and stack entries use 64 bits rather than 32 bits. This makes data caching less efficient.
</li>
<li>Access to static or global arrays require a few extra instructions for address calculation in 64 bit mode if the image base is not guaranteed to be less than 2^31. This extra cost is seen in 64 bit Windows and Mac programs but rarely in Linux.
</li>
<li>Address calculation is more complicated in a large memory model where the combined size of code and data can exceed 2 Gbytes. This large memory model is hardly ever used, though.
</li>
<li>Some instructions are one byte longer in 64 bit mode than in 32 bit mode.
</li>
<li>Some 64-bit compilers are inferior to their 32-bit counterparts.
</li>
</ul>

<p>
64位的Linux会比Windows使用更多的寄存器来传递函数参数，可是为什么Windows不这么做呢？
</p>

<p>
The similarity between the operating systems disappears when running in 64-bit mode because the function calling conventions are different. 64-bit Windows allows only four function parameters to be transferred in registers, whereas 64-bit Linux, BSD and Mac allow up to fourteen parameters to be transferred in registers (6 integer and 8 floating point). There are also other details that make function calling more efficient in 64-bit Linux than in 64-bit Windows (See page 49 and manual 5: "Calling conventions for different C++ compilers and operating systems"). An application with many function calls may run slightly faster in 64-bit Linux than in 64-bit Windows. The disadvantage of 64-bit Windows may be mitigated by making critical functions inline or static or by using a compiler that can do whole program optimization.
</p>

<p>
使用平台独立中间语言（Java Bytecode，CIL）好处是代码会更加紧凑。但是这个紧凑仅仅是对于发布而言，而不是对于运行时而言。使用JIT可以有效地提升性能，但是JIT的runtime本身开销就比较大。此外因为托管代码的引入抽象层，而JIT本身行为也非常复杂，所以造成手动地对进行某种优化会非常困难。
</p>

<p>
The reason for using an intermediate code is that it is intended to be platform-independent and compact. The biggest disadvantage of using an intermediate code is that the user must install a large runtime framework for interpreting or compiling the intermediate code. This framework typically uses much more resources than the code itself.
</p>

<p>
Another disadvantage of intermediate code is that it adds an extra level of abstraction which makes detailed optimization more difficult. On the other hand, a just-in-time compiler can optimize specifically for the CPU it is running on, while it is more complicated to make CPU- specific optimizations in precompiled code.
</p>

<p>
在Visual Studio上C++也可以被编译成为托管代码！！
</p>

<p>
Development in C++ is quite efficient thanks to the availability of powerful development tools. One popular development tool is Microsoft Visual Studio. This tool can make two different implementations of C++, directly compiled code and intermediate code for the common language runtime of the .NET framework. Obviously, the directly compiled version is preferred when speed is important.
</p>

<p>
对于Intel这种行为感到强烈鄙视！！
</p>

<p>
The compiler supports automatic CPU dispatching to make multiple code versions for different Intel CPUs. The most important disadvantage of the Intel compiler is that the code is optimized for a specific Intel CPU. The compiled code checks whether it is running on an Intel CPU. If another brand of CPU (AMD or VIA) is detected, then it will run a different branch of the code with reduced speed, even if the optimal code branch is compatible with the processor it is running on. It is possible to avoid this problem in some cases by bypassing the CPU-dispatcher that checks whether the code is running on an Intel CPU. See page 139 for details).
</p>

<p>
作者自己编写的库函数集合，可以作为学习材料
</p>

<p>
My own function library made for demonstration purposes. Available from <a href="http://www.agner.org/optimize/asmlib.zip">http://www.agner.org/optimize/asmlib.zip</a>. Currently includes optimized versions of memory and string functions and some other functions that are difficult to find elsewhere. Faster than most other libraries when running on the newest processors. Supports all x86 and x86-64 platforms.
</p>
</div>
</div>

<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> 3. Finding the biggest time consumers</h2>
<div class="outline-text-2" id="text-3">
<p>
下面是几种性能剖分方式：指令，调试，时间采样，事件采样
</p>

<p>
There are several different profiling methods:
</p>
<ul class="org-ul">
<li>Instrumentation: The compiler inserts extra code at each function call to count how many times the function is called and how much time it takes.
</li>
<li>Debugging. The profiler inserts temporary debug breakpoints at every function or every code line.
</li>
<li>Time-based sampling: The profiler tells the operating system to generate an interrupt, e.g. every millisecond. The profiler counts how many times an interrupt occurs in each part of the program. This requires no modification of the program under test, but is less reliable.
</li>
<li>Event-based sampling: The profiler tells the CPU to generate interrupts at certain events, for example every time a thousand cache misses have occurred. This makes it possible to see which part of the program has most cache misses, branch mispredictions, floating point exceptions, etc. Event-based sampling requires a CPU-specific profiler. For Intel CPUs use Intel VTune, for AMD CPUs use AMD CodeAnalyst.
</li>
</ul>


<p>
注意区分吞吐量和延迟，对于CPU来说也是一样。在加入了流水线之后（尤其是深度流水），指令的延迟和吞吐量就出现了偏离。一个指令可能会执行10个周期，但是如果前后指令不出现依赖的话，那么理论上实际的吞吐量可以达到一个指令1周期。乱序执行以及分支预测技术，也是为了提高流水线效率，而被设计出来的。
</p>

<p>
There is an important distinction between the latency and the throughput of an execution unit. For example, it may take 3 - 5 clock cycles to do a floating point addition on a modern CPU. But it is possible to start a new floating point addition every clock cycle. This means that if each addition depends on the result of the preceding addition then you will have only one addition every three clock cycles. But if all the additions are independent then you can have one addition every clock cycle.
</p>

<p>
现代CPU执行单元被分成为了许多部分，比如整数计算，浮点加法，浮点乘法等等，并且可能这些部分还会存在冗余。这些部分是相互独立的，为了提升并行度，在指令上我们最好可以将整数计算和浮点计算进行搭配，来有效使用CPU。
</p>

<p>
The execution core of modern microprocessors is split between several execution units. Typically, there are two or more integer units, one or two floating point addition units, and one or two floating point multiplication units. This means that it is possible to do an integer addition, a floating point addition, and a floating point multiplication at the same time.
</p>

<p>
A code that does floating point calculations should therefore preferably have a balanced mix of additions and multiplications. Subtractions use the same unit as additions. Divisions take longer time. It is possible to do integer operations in-between the floating point operations without reducing the performance because the integer operations use different execution units. For example, a loop that does floating point calculations will typically use integer operations for incrementing a loop counter, comparing the loop counter with its limit, etc. In most cases, you can assume that these integer operations do not add to the total computation time.
</p>
</div>
</div>

<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> 4. Performance and usability</h2>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> 5. Choosing the optimal algorithm</h2>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> 6. Development process</h2>
</div>
<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> The efficiency of different C++ constructs</h2>
<div class="outline-text-2" id="text-7">
<p>
关于浮点寄存器的变化。最开始intel提供的是x87浮点协处理器，操作方式是栈式的，所以好处是指令比较精简，但是缺点就是对编译器的限制太大。x87寄存器是80bits的，精度上没有任何问题，并且提供了许多内置的数学计算指令，在浮点本身精度转换上效率可以，但是在整数和浮点转换上效率不好。
</p>

<p>
Modern microprocessors in the x86 family have two different types of floating point registers and correspondingly two different types of floating point instructions. Each type has advantages and disadvantages.
</p>

<p>
The original method of doing floating point operations involves eight floating point registers organized as a register stack. These registers have long double precision (80 bits). The advantages of using the register stack are:
</p>
<ul class="org-ul">
<li>All calculations are done with long double precision.
</li>
<li>Conversions between different precisions take no extra time.
</li>
<li>There are intrinsic instructions for mathematical functions such as logarithms and trigonometric functions.
</li>
<li>The code is compact and takes little space in the code cache.
</li>
</ul>
<p>
The register stack also has disadvantages:
</p>
<ul class="org-ul">
<li>It is difficult for the compiler to make register variables because of the way the register stack is organized.
</li>
<li>Floating point comparisons are slow unless the Pentium-II or later instruction set is enabled.
</li>
<li>Conversions between integers and floating point numbers is inefficient.
</li>
<li>Division, square root and mathematical functions take more time to calculate when long double precision is used.
</li>
</ul>

<p>
在出了SSE之后，提供了一套和整形寄存器类似使用方式的寄存器组(XMM,YMM,ZMM)，这些寄存器尺寸很大所以适合做并行计算。
</p>

<p>
A newer method of doing floating point operations involves eight or more vector registers (XMM, YMM, or ZMM) which can be used for multiple purposes. Floating point operations are done with single or double precision, and intermediate results are always calculated with the same precision as the operands. The advantages of using the vector registers are:
</p>
<ul class="org-ul">
<li>It is easy to make floating point register variables.
</li>
<li>Vector operations are available for doing parallel calculations on multiple variables in vector registers (see page 112).
</li>
</ul>
<p>
Disadvantages are:
</p>
<ul class="org-ul">
<li>Long double precision is not supported.
</li>
<li>The calculation of expressions where operands have mixed precision require precision conversion instructions which can be quite time-consuming (see page 150).
</li>
<li>Mathematical functions must use a function library, but this is often faster than the intrinsic hardware functions.
</li>
</ul>

<p>
The x87 floating point registers are available in all systems that have floating point capabilities (except in device drivers for 64-bit Windows). The XMM, YMM, and ZMM registers require the SSE, AVX, and AVX512 instruction set, respectively. See page 131 for how to test for the availability of these instruction sets.
</p>

<p>
关于单精度和双精度浮点的选择上，主要还是依照精度本身而定。单精度在某些操作上会快些，并且因为体积更小缓存效果更好。浮点加法在3-6周期，乘法4-8周期，除法14-45周期，类型转换在2-15周期（除非使用x87, 但是应该也不怎么用了吧）。有符号整数转为浮点使用4-16个周期，而浮点转回去则需要使用50-100个周期，之所以这么大是因为C/C++转换使用截断而非round，如果使用汇编进行round的话，那么速度会快不少（也就是说，不要用C/C++内置的浮点到整形的类型转换）。
</p>

<p>
In most cases, double precision calculations take no more time than single precision as long as they are not joined into vectors. Single precision division, square root, and mathematical functions are calculated faster than double precision when the XMM registers are used, while the speed of addition, subtraction, multiplication, etc. is still the same regardless of precision on most processors when vector operations are not used.
</p>

<p>
You may use double precision without worrying too much about the costs if it is good for the application. You may use single precision if you have big arrays and want to get as much data as possible into the data cache. Single precision is good if you can take advantage of vector operations, as explained on page 112.
</p>

<p>
Floating point addition takes 3 - 6 clock cycles, depending on the microprocessor. Multiplication takes 4 - 8 clock cycles. Division takes 14 - 45 clock cycles. Floating point comparisons and floating point to integer conversions are inefficient when the old x87 floating point registers are used.
</p>

<p>
Conversions between float, double and long double take no extra time when the floating point register stack is used. It takes between 2 and 15 clock cycles (depending on the processor) when the XMM registers are used.
</p>

<p>
Conversion of a signed integer to a float or double takes 4 - 16 clock cycles, depending on the processor and the type of registers used. Conversion of an unsigned integer takes longer time. It is faster to first convert the unsigned integer to a signed integer if there is no risk of overflow.
</p>

<p>
Conversion of a floating point number to an integer takes a very long time unless the SSE2 or later instruction set is enabled. Typically, the conversion takes 50 - 100 clock cycles. The reason is that the C/C++ standard specifies truncation so the floating point rounding mode has to be changed to truncation and back again. Use rounding instead of truncation and make a round function using assembly language. See page 150 for details about rounding.
</p>

<p>
分支预测正确跳转占用0-2个时钟周期，而错误的话占用12-25个时钟周期。为了加快分支跳转，有个特殊的cache叫做branch target buffer. 但是因为容量有效，所以如果分支或者是函数调用特别多的话，那么因为冲突也会造成性能损失，所以在关键代码部分要减少分支和函数调用。
</p>

<p>
A branch instruction takes typically 0 - 2 clock cycles in the case that the microprocessor has made the right prediction. The time it takes to recover from a branch misprediction is approximately 12 - 25 clock cycles, depending on the processor. This is called the branch misprediction penalty.
</p>

<p>
The target of branches and function calls are saved in a special cache called the branch target buffer. Contentions in the branch target buffer can occur if a program has many branches or function calls. The consequence of such contentions is that branches can be mispredicted even if they otherwise would be predicted well. Even direct function calls can be mispredicted for this reason. A program with many branches and function calls in the critical part of the code can therefore suffer from mispredictions.
</p>

<p>
64位系统下面Linux和Windows的传参方式对比。相比32位，64位有更多的寄存器可以用来传递参数。
</p>

<p>
Parameter transfer is more efficient in 64-bit mode than in 32-bit mode, and more efficient in 64-bit Linux than in 64-bit Windows. In 64-bit Linux, the first six integer parameters and the first eight floating point parameters are transferred in registers, totaling up to fourteen register parameters. In 64-bit Windows, the first four parameters are transferred in registers, regardless of whether they are integers or floating point numbers. Therefore, 64-bit Linux is more efficient than 64-bit Windows if functions have more than four parameters. There is no difference between 32-bit Linux and 32-bit Windows in this respect.
</p>

<p>
The number of registers available for floating point and vector variables is 8 registers in 32- bit mode, and 16 registers in 64-bit mode. It is further increased to 32 registers in 64 bit mode when the AVX512 instruction set is enabled. A high number of registers improves the performance because the compiler can store variables in registers rather than in memory.
</p>

<p>
上下文切换开销是显著的。系统在会分配给前台任务时间片是30ms，后台任务时间片10ms。这个数量级别可以作为参考。
</p>

<p>
Threads are used for doing two or more jobs simultaneously or seemingly simultaneously. Modern CPUs have multiple cores that makes it possible to run multiple threads simultaneously. Each thread will get time slices of typically 30 ms for foreground jobs and 10 ms for background jobs when there are more threads than CPU cores. The context switches after each time slice are quite costly because all caches have to adapt to the new context. It is possible to reduce the number of context switches by making longer time slices. This will make applications run faster at the cost of longer response times for user input.
</p>
</div>
</div>

<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> Optimizations in the compiler</h2>
<div class="outline-text-2" id="text-8">
</div><div id="outline-container-sec-8-1" class="outline-3">
<h3 id="sec-8-1"><span class="section-number-3">8.1</span> How compilers optimize</h3>
<div class="outline-text-3" id="text-8-1">
<ul class="org-ul">
<li>Function inlining
</li>
<li>Constant folding and constant propagation
</li>
<li>Pointer elimination
</li>
<li>Common subexpression elimination
</li>
<li>Register variables
<ul class="org-ul">
<li>The maximum number of integer register variables is approximately six in 32-bit systems and fourteen in 64-bit systems.
</li>
<li>The maximum number of floating point register variables is eight in 32-bit systems and sixteen in 64-bit systems.
</li>
<li>Some compilers have difficulties making floating point register variables in 32-bit systems unless the SSE2 (or later) instruction set is enabled.
</li>
</ul>
</li>
<li>Live range analysis
</li>
<li>Join identical branches
</li>
<li>Eliminate jumps
</li>
<li>Loop unrolling
</li>
<li>Loop invariant code motion
</li>
<li>Induction variables
</li>
<li>Scheduling
</li>
<li>Algebraic reductions
</li>
<li>Devirtualization
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-8-2" class="outline-3">
<h3 id="sec-8-2"><span class="section-number-3">8.2</span> Comparison of different compilers</h3>
</div>
<div id="outline-container-sec-8-3" class="outline-3">
<h3 id="sec-8-3"><span class="section-number-3">8.3</span> Obstacles to optimization by compiler</h3>
<div class="outline-text-3" id="text-8-3">
<ul class="org-ul">
<li>Cannot optimize across modules
</li>
<li>Pointer aliasing
<ul class="org-ul">
<li>It is also possible to tell the compiler that a specific pointer does not alias anything by using the keyword <span class="underline"><span class="underline">restrict or __restrict</span></span>, if supported by the compiler.
</li>
<li>We can never be sure that the compiler takes the hint about no pointer aliasing. The only way to make sure that the code is optimized is to do it explicitly.
</li>
</ul>
</li>
<li>Dynamic memory allocation
</li>
<li>Pure functions
<ul class="org-ul">
<li>Unfortunately, the compiler cannot know that a function is pure if the function is defined in a different module or a function library.
</li>
<li>__attribute__((const))
</li>
</ul>
</li>
<li>Virtual functions and function pointers
</li>
<li>Algebraic reduction
</li>
<li>Floating point induction variables
</li>
<li>Inlined functions have a non-inlined copy
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-8-4" class="outline-3">
<h3 id="sec-8-4"><span class="section-number-3">8.4</span> Obstacles to optimization by CPU</h3>
</div>
<div id="outline-container-sec-8-5" class="outline-3">
<h3 id="sec-8-5"><span class="section-number-3">8.5</span> Compiler optimization options</h3>
<div class="outline-text-3" id="text-8-5">
<p>
Some compilers have support for whole program optimization. This works by compiling in
two steps. All source files are first compiled to an intermediate file format instead of the
usual object file format. The intermediate files are then linked together in the second step
where the compilation is finished. Register allocation and function inlining is done at the
second step. The intermediate file format is not standardized. It is not even compatible with
different versions of the same compiler. It is therefore not possible to distribute function
libraries in this format.（程序整体优化使用中间格式而不是使用目标文件格式）
</p>

<p>
The code becomes more efficient when there is no exception handling. It is recommended
to turn off support for exception handling unless the code relies on structured exception
handling and you want the code to be able to recover from exceptions.（关闭异常处理）
</p>

<p>
It is recommended to turn off support for runtime type identification (RTTI)（关闭RTTI）
</p>

<p>
It is recommended to enable fast floating point calculations or turn off requirements for strict
floating point calculations unless the strictness is required.（快速浮点运算关闭严格执行模式）
</p>

<p>
Use the option for "assume no pointer aliasing" if you are sure the code has no pointer
aliasing.（如果确认没有指针别名的话，那么打开“假设没有指针别名”的编译选项）
</p>

<p>
Many compilers have an option for "standard stack frame" or "frame pointer". The standard
stack frame is used for debugging and exception handling. Omitting the standard stack
frame makes function calls faster and makes an extra register available for other purposes.
This is advantageous because registers is a scarce resource. Do not use a stack frame
unless your program relies on exception handling.（对帧指针不分配寄存器。帧指针在调试以及异常处理的时候会使用到）
</p>
</div>
</div>

<div id="outline-container-sec-8-6" class="outline-3">
<h3 id="sec-8-6"><span class="section-number-3">8.6</span> Optimization directives</h3>
</div>
<div id="outline-container-sec-8-7" class="outline-3">
<h3 id="sec-8-7"><span class="section-number-3">8.7</span> Checking what the compiler does</h3>
</div>
</div>

<div id="outline-container-sec-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> Optimizing memory access</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-sec-9-1" class="outline-3">
<h3 id="sec-9-1"><span class="section-number-3">9.1</span> Caching of code and data</h3>
</div>
<div id="outline-container-sec-9-2" class="outline-3">
<h3 id="sec-9-2"><span class="section-number-3">9.2</span> Cache organization</h3>
</div>
<div id="outline-container-sec-9-3" class="outline-3">
<h3 id="sec-9-3"><span class="section-number-3">9.3</span> Functions that are used together should be stored together</h3>
</div>
<div id="outline-container-sec-9-4" class="outline-3">
<h3 id="sec-9-4"><span class="section-number-3">9.4</span> Variables that are used together should be stored together</h3>
</div>
<div id="outline-container-sec-9-5" class="outline-3">
<h3 id="sec-9-5"><span class="section-number-3">9.5</span> Alignment of data</h3>
</div>
<div id="outline-container-sec-9-6" class="outline-3">
<h3 id="sec-9-6"><span class="section-number-3">9.6</span> Dynamic memory allocation</h3>
<div class="outline-text-3" id="text-9-6">
<p>
A little-known alternative to using new and delete is to allocate variable-size arrays with
alloca. This is a function that allocates memory on the stack rather than the heap. The
space is automatically deallocated when returning from the function in which alloca was
called. There is no need to deallocate the space explicitly when alloca is used.
（使用alloca可以在栈上开辟空间，但是需要防止栈溢出）
</p>
</div>
</div>

<div id="outline-container-sec-9-7" class="outline-3">
<h3 id="sec-9-7"><span class="section-number-3">9.7</span> Container classes</h3>
</div>
<div id="outline-container-sec-9-8" class="outline-3">
<h3 id="sec-9-8"><span class="section-number-3">9.8</span> Strings</h3>
</div>
<div id="outline-container-sec-9-9" class="outline-3">
<h3 id="sec-9-9"><span class="section-number-3">9.9</span> Access data sequentially</h3>
</div>
<div id="outline-container-sec-9-10" class="outline-3">
<h3 id="sec-9-10"><span class="section-number-3">9.10</span> Cache contentions in large data structures</h3>
</div>
<div id="outline-container-sec-9-11" class="outline-3">
<h3 id="sec-9-11"><span class="section-number-3">9.11</span> Explicit cache control</h3>
<div class="outline-text-3" id="text-9-11">
<p>
包括预取指令（如果不是使用常规访问模式来访问内存的话）以及”写内存但是不写缓存“指令（如果确定数据之后不会读取上来并且cache冲突严重）
</p>
</div>
</div>
</div>

<div id="outline-container-sec-10" class="outline-2">
<h2 id="sec-10"><span class="section-number-2">10</span> Multithreading</h2>
<div class="outline-text-2" id="text-10">
<p>
It is important to distinguish between coarse-grained parallelism and fine-grained parallelism
when deciding whether it is advantageous to do things in parallel. Coarse-grained
parallelism refers to the situation where a long sequence of operations can be carried out
independently of other tasks that are running in parallel. Fine-grained parallelism is the
situation where a task is divided into many small subtasks, but it is impossible to work for
very long on a particular subtask before coordination with other subtasks is necessary.（粗粒度和细粒度并行）
</p>

<p>
Multithreading works more efficiently with coarse-grained parallelism than with fine-grained
parallelism because communication and synchronization between the different cores is
slow. If the granularity is too fine then it is not advantageous to split the tasks into multiple
threads. Out-of-order execution (chapter 11) and vector operations (chapter 12) are more
useful methods for exploiting fine-grained parallelism. （多线程适合解决粗粒度并行工作，OOO以及向量操作适合解决细粒度并行工作）
</p>

<p>
In the case of data decomposition, we should preferably
have no more threads with the same priority than the number of cores or logical processors
available in the system. The number of logical processors available can be determined by a
system call (e.g. GetProcessAffinityMask in Windows).（理想情况线程数目和逻辑/物理CPU core数目相同并且有相同优先级别）
</p>

<p>
The multiple CPU cores or logical processors usually share the same cache, at least at the
last cache level, and in some cases even the same level-1 cache. The advantage of sharing
the same cache is that communication between threads becomes faster and that threads
can share the same code and read-only data. The disadvantage is that the cache will be
filled up if the threads use different memory areas, and there will be cache contentions if the
threads write to the same memory areas.（共享cache可以方便数据交换，但是也会造成cache冲突）
</p>

<p>
It is not good to have two or more threads
writing to the same cache line, because the threads will invalidate each other's caches and
cause large delays. The easiest way to make thread-specific data is to declare it locally in
the thread function so that it is stored on the stack. Each thread has its own stack.
Alternatively, you may define a structure or class for containing thread-specific data and
make one instance for each thread. This structure or class should be aligned by at least the
cache line size in order to avoid multiple threads writing to the same cache line. The cache
line size is typically 64 bytes on contemporary processors. The cache line size may possibly
be more (128 or 256 bytes) on future processors. （现代处理器的cache line典型值是64字节，未来可能扩展到128和256字节）
</p>
</div>

<div id="outline-container-sec-10-1" class="outline-3">
<h3 id="sec-10-1"><span class="section-number-3">10.1</span> Hyperthreading</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Some versions of Intel microprocessors are able to run two threads in each core. For
example, a Core i7 processor with four cores can run eight threads simultaneously. This
processor has four physical processors but eight logical processors.（物理处理器和虚拟处理器）
</p>

<p>
Hyperthreading is Intel's term for running multiple threads in the same processor core. Two
threads running in the same core will always compete for the same resources, such as
cache and execution units. If any of the shared resources are limiting factors for the
performance then there is no advantage to using hyperthreading. On the contrary, each
thread may run at less than half speed because of cache evictions and other resource
conflicts. But if a large fraction of the time goes to cache misses, branch misprediction, or
long dependency chains then each thread will run at more than half the single-thread speed.
In this case there is an advantage to using hyperthreading, but the performance is not
doubled. A thread that shares the resources of the core with another thread will always run
slower than a thread that runs alone in the core.（如果竞争共享资源比较激烈的话，那么使用超线程没有任何好处。
相反如果资源消耗主要在非共享资源上的话那么使用超线程可以加快速度，但是性能通常不会翻倍）
</p>

<p>
It is often necessary to do experiments in order to determine whether it is advantageous to
use hyperthreading or not in a particular application.（是否使用超线程需要根据应用情况来定）
</p>

<p>
If hyperthreading is not advantageous then it is necessary to query certain operating system
functions (e.g. GetLogicalProcessorInformation in Windows) to determine if the
processor has hyperthreading. If so, then you can avoid hyperthreading by using only the
even-numbered logical processors (0, 2, 4, etc.). Older operating systems lack the
necessary functions for distinguishing between the number of physical processors and the
number of logical processors.（如果支持超线程的话那么可以只使用偶数编号处理器可以避免使用超线程）
</p>

<p>
There is no way to tell a hyperthreading processor to give higher priority to one thread than
another. Therefore, it can often happen that a low-priority thread steals resources from a
higher-priority thread running in the same core. It is the responsibility of the operating
system to avoid running two threads with widely different priority in the same processor
core. Unfortunately, contemporary operating systems are not always avoiding this.（操作系统来处理超线程
处理器上超线程优先级别之间的关系）
</p>

<p>
The Intel compiler is capable of making two threads where one thread is used for
prefetching data for the other thread. However, in most cases you can rely on automatic
prefetching so this feature is rarely needed.（大部分情况使用默认CPU预取机制就足够）
</p>
</div>
</div>
</div>

<div id="outline-container-sec-11" class="outline-2">
<h2 id="sec-11"><span class="section-number-2">11</span> Out of order execution</h2>
<div class="outline-text-2" id="text-11">
<p>
All modern x86 CPUs can execute instructions out of order or do more than one thing at the
same time（现在X86 cpu允许OOO来使得在同一个时间完成多项任务）
</p>

<p>
Calculations in a loop where each iteration needs the result of the preceding one is called a
loop-carried dependency chain. Such dependency chains can be very long and very time-
consuming. There is a lot to gain if such dependency chains can be broken up.
</p>

<p>
It is not necessary to unroll a loop and use multiple accumulators if there is no loop-carried
dependency chain. A microprocessor with out-of-order capabilities can overlap the iterations
and start the calculation of one iteration before the preceding iteration is finished. Example:
</p>
<div class="org-src-container">

<pre class="src src-C++"><span class="org-comment-delimiter">// </span><span class="org-comment">Example 11.3</span>
<span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">size</span> = 100; <span class="org-type">int</span> <span class="org-variable-name">i</span>;
<span class="org-type">float</span> <span class="org-variable-name">a</span>[size], <span class="org-variable-name">b</span>[size], <span class="org-variable-name">c</span>[size];
<span class="org-type">float</span> <span class="org-keyword">register</span> temp;
<span class="org-keyword">for</span> (i = 0; i &lt; size; i++) {
  temp = a[i] + b[i];
  c[i] = temp * temp;
}
</pre>
</div>
<p>
Microprocessors with out-of-order capabilities are very smart. They can detect that the value
of register temp in one iteration of the loop in example 11.3 is independent of the value in
the previous iteration. This allows it to begin calculating a new value of temp before it is
finished using the previous value. It does this by assigning a new physical register to temp
even though the logical register that appears in the machine code is the same. This is called
register renaming. The CPU can hold many renamed instances of the same logical register.
（如果没有loop-carried dependency chain的话，那么没有必要做循环展开）
</p>

<p>
This advantage comes automatically. There is no reason to unroll the loop and have a
temp1 and temp2. Modern CPUs are capable of register renaming and doing multiple
calculations in parallel if certain conditions are satisfied. The conditions that make it possible
for the CPU to overlap the calculations of loop iterations are:（通常满足下面这些条件的话CPU可以将多个循环迭代交叠）
</p>
<ul class="org-ul">
<li>No loop-carried dependency chain. Nothing in the calculation of one iteration should depend on the result of the previous iteration (except for the loop counter, which is calculated fast if it is an integer)（没有每轮循环之间的相互依赖）
</li>
<li>All intermediate results should be saved in registers, not in memory. The renaming mechanism works only on registers, not on variables in memory or cache. Most compilers will make temp a register variable in example 11.3 even without the register keyword.（所有中间结果存放在寄存器）（自动完成）
</li>
<li>The loop branch should be predicted. This is no problem if the repeat count is large or constant. If the loop count is small and changing then the CPU may occasionally predict that the loop exits, when in fact it does not, and therefore fail to start the next calculation. However, the out-of-order mechanism allows the CPU to increment the loop counter ahead of time so that it may detect the misprediction before it is too late. You should therefore not be too worried about this condition.（开启循环分支预判功能）（自动完成）
</li>
</ul>

<p>
In general, the out-of-order execution mechanism works automatically. However, there are a
couple of things that the programmer can do to take maximum advantage of out-of-order
execution. The most important thing is to avoid long dependency chains. Another thing that
you can do is to mix different kinds of operations in order to divide the work evenly between
the different execution units in the CPU. It can be advantageous to mix integer and floating
point calculations as long as you don't need conversions between integers and floating point
numbers. It can also be advantageous to mix floating point addition with floating point
multiplication, to mix simple integer with vector integer operations, and to mix mathematical
calculations with memory access.（除了打破dependency chain之外，还可以通过混合不同类型的计算来获得OOO的好处）
</p>
</div>
</div>

<div id="outline-container-sec-12" class="outline-2">
<h2 id="sec-12"><span class="section-number-2">12</span> Using vector operations</h2>
<div class="outline-text-2" id="text-12">
<p>
#note: 之前调研过x86 simd指令集并且整理过一篇<a href="simd.html">文章</a>
</p>
</div>
</div>

<div id="outline-container-sec-13" class="outline-2">
<h2 id="sec-13"><span class="section-number-2">13</span> Making critical code in multiple versions for different instruction sets</h2>
<div class="outline-text-2" id="text-13">
<p>
A disadvantage of using the newest instruction set is that the compatibility with older
microprocessors is lost. This dilemma can be solved by making the most critical parts of the
code in multiple versions for different CPUs. This is called CPU dispatching. For example,
you may want to make one version that takes advantage of the AVX instruction set, another
version for CPUs with only the SSE2 instruction set, and a generic version that is
compatible with old microprocessors without any of these instruction sets. The program
should automatically detect which instruction set is supported by the CPU and the operating
system and choose the appropriate version of the subroutine for the critical innermost loops.
(使用CPU分派技术来使用和兼容不同指令集合或者CPU型号）
</p>
</div>
</div>

<div id="outline-container-sec-14" class="outline-2">
<h2 id="sec-14"><span class="section-number-2">14</span> Specific optimization topics</h2>
<div class="outline-text-2" id="text-14">
</div><div id="outline-container-sec-14-1" class="outline-3">
<h3 id="sec-14-1"><span class="section-number-3">14.1</span> Use lookup tables</h3>
<div class="outline-text-3" id="text-14-1">
<p>
Replacing a function with a lookup table is advantageous in most cases where the number
of possible inputs is limited and there are no cache problems. It is not advantageous to use
a lookup table if you expect the table to be evicted from the cache between each call, and
the time it takes to calculate the function is less than the time it takes to reload the value
from memory plus the costs to other parts of the program of occupying a cache line.（重新计算和表格cache miss相比）
</p>

<p>
Table lookup cannot be vectorized with the current instruction set. Do not use lookup tables
if this prevents a faster vectorized code.（向量化代码）
</p>

<p>
Storing something in static memory can cause caching problems because static data are
likely to be scattered around at different memory addresses. If caching is a problem then it
may be useful to copy the table from static memory to stack memory outside the innermost
loop.（静态内存分布在不同的内存区域上，容易造成cache miss. 存放在栈上可以缓解这个问题）
</p>
</div>
</div>

<div id="outline-container-sec-14-2" class="outline-3">
<h3 id="sec-14-2"><span class="section-number-3">14.2</span> Bounds checking</h3>
<div class="outline-text-3" id="text-14-2">
<div class="org-src-container">

<pre class="src src-C++"><span class="org-keyword">if</span> (i &lt; 0 || i &gt;= size) {
  cout &lt;&lt; <span class="org-string">"Error: Index out of range"</span>;
}
<span class="org-comment-delimiter">// </span><span class="org-comment">TO</span>
<span class="org-keyword">if</span> ((<span class="org-type">unsigned</span> <span class="org-type">int</span>)i &gt;= (<span class="org-type">unsigned</span> <span class="org-type">int</span>)size) {
  cout &lt;&lt; <span class="org-string">"Error: Index out of range"</span>;
}

<span class="org-keyword">if</span> (i &gt;= min &amp;&amp; i &lt;= max) { ... }
<span class="org-comment-delimiter">// </span><span class="org-comment">TO</span>
<span class="org-keyword">if</span> ((<span class="org-type">unsigned</span> <span class="org-type">int</span>)(i - min) &lt;= (<span class="org-type">unsigned</span> <span class="org-type">int</span>)(max - min)) { ...
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-14-3" class="outline-3">
<h3 id="sec-14-3"><span class="section-number-3">14.3</span> Use bitwise operators for checking multiple values at once</h3>
</div>
<div id="outline-container-sec-14-4" class="outline-3">
<h3 id="sec-14-4"><span class="section-number-3">14.4</span> Integer multiplication</h3>
<div class="outline-text-3" id="text-14-4">
<p>
Integer multiplication takes longer time than addition and subtraction (3 - 10 clock cycles,
depending on the processor).（整数乘法通常在3-10个时钟周期）
</p>

<div class="org-src-container">

<pre class="src src-C++"><span class="org-keyword">struct</span> <span class="org-type">S1</span> {
  <span class="org-type">int</span> <span class="org-variable-name">a</span>;
  <span class="org-type">int</span> <span class="org-variable-name">b</span>;
  <span class="org-type">int</span> <span class="org-variable-name">c</span>;
  <span class="org-type">int</span> <span class="org-variable-name">UnusedFiller</span>;
};
<span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">size</span> = 100;
<span class="org-type">S1</span> <span class="org-variable-name">list</span>[size];
</pre>
</div>
<p>
通过增加UnusedFiller字段来使得结构体大小是2^n. 这样从下标偏移对应到内存偏移计算相对就更快速。
</p>

<p>
The advise of using powers of 2 does not apply to very big data structures. On the contrary,
you should by all means avoid powers of 2 if a matrix is so big that caching becomes a
problem. If the number of columns in a matrix is a power of 2 and the matrix is bigger than
the cache then you can get very expensive cache contentions, as explained on page 96.
（但是上面的方法不适合大的数据结构体。因为cache冲突导致cache miss所带来的penalty相比整数乘法而言更大）
</p>
</div>
</div>

<div id="outline-container-sec-14-5" class="outline-3">
<h3 id="sec-14-5"><span class="section-number-3">14.5</span> Integer division</h3>
<div class="outline-text-3" id="text-14-5">
<p>
Integer division takes much longer time than addition, subtraction and multiplication (27 - 80
clock cycles for 32-bit integers, depending on the processor).（整数除法通常在27-80个时钟周期）
</p>

<p>
The following guidelines can be used for improving code that contains integer division:
</p>
<ul class="org-ul">
<li>Integer division by a constant is faster than division by a variable
</li>
<li>Integer division by a constant is faster if the constant is a power of 2
</li>
<li>Integer division by a constant is faster if the dividend is unsigned
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-14-6" class="outline-3">
<h3 id="sec-14-6"><span class="section-number-3">14.6</span> Floating point division</h3>
<div class="outline-text-3" id="text-14-6">
<p>
Floating point division takes much longer time than addition, subtraction and multiplication
(20 - 45 clock cycles). （浮点数除法在20-45个时钟周期，远超过加减乘，所以如果可以的话那么尽量使用乘法代替）
</p>
</div>
</div>

<div id="outline-container-sec-14-7" class="outline-3">
<h3 id="sec-14-7"><span class="section-number-3">14.7</span> Don't mix float and double</h3>
<div class="outline-text-3" id="text-14-7">
<p>
Floating point calculations usually take the same time regardless of whether you are using
single precision or double precision, but there is a penalty for mixing single and double
precision in programs compiled for 64-bit operating systems and programs compiled for the
instruction set SSE2 or later.（通常来说保持使用单精度或者是多精度所耗费的时间是相同的，但是如果在64位操作系统
或程序下使用SSE2以及后续指令来混合操作两者的话，那么会存在额外开销）
</p>

<p>
There is no penalty for mixing different floating point precisions when the code is compiled
for old processors without the SSE2 instruction set, but it may be preferable to keep the
same precision in all operands in case the code is later ported to another platform.
</p>
</div>
</div>

<div id="outline-container-sec-14-8" class="outline-3">
<h3 id="sec-14-8"><span class="section-number-3">14.8</span> Conversions between floating point numbers and integers</h3>
<div class="outline-text-3" id="text-14-8">
<p>
<b>Conversion from floating point to integer</b>
</p>

<p>
According to the standards for the C++ language, all conversions from floating point
numbers to integers use truncation towards zero, rather than rounding. This is unfortunate
because truncation takes much longer time than rounding unless the SSE2 instruction set is
used. It is recommended to enable the SSE2 instruction set if possible. SSE2 is always
enabled in 64-bit mode.（C++标准要求浮点转整形是截断而不是舍入，而截断只有在SSE2指令上才能表现良好。
不过在64位下SSE2模式是打开的，所以我们这里主要考虑32位系统）
</p>

<p>
A conversion from floating point to integer without SSE2 typically takes 40 clock cycles. If
you cannot avoid conversions from float or double to int in the critical part of the
code, then you may improve efficiency by using rounding instead of truncation. This is
approximately three times faster. The logic of the program may need modification to
compensate for the difference between rounding and truncation.
（在不使用SSE2情况下，截断使用40个指令周期，而舍入则使用13个指令周期。舍入函数是lrint和lrintf）
</p>

<p>
In 64-bit mode or when the SSE2 instruction set is enabled there is no difference in speed
between rounding and truncation.
</p>

<p>
<b>Conversion from integer to floating point</b>
</p>

<p>
Conversion of integers to floating point is faster than from floating point to integer. The
conversion time is typically between 5 and 20 clock cycles. It may in some cases be
advantageous to do simple integer calculations in floating point variables in order to avoid
conversions from integer to floating point.（占用5-20个时钟周期。所以有时候可以在浮点数上做一些简单的整数操作
来避免整形向浮点数的转换）
</p>

<p>
Conversion of unsigned integers to floating point numbers is less efficient than signed
integers. It is more efficient to convert unsigned integers to signed integers before
conversion to floating point if the conversion to signed integer doesn't cause overflow.
（从有符号数转向浮点数，相比无符号数更快）
</p>
</div>
</div>

<div id="outline-container-sec-14-9" class="outline-3">
<h3 id="sec-14-9"><span class="section-number-3">14.9</span> Using integer operations for manipulating floating point variables</h3>
<div class="outline-text-3" id="text-14-9">
<p>
The representation of float, double and long double reflects the floating point value
written as (+-)2^eee * 1.fffff, where ± is the sign, eee is the exponent, and fffff is the
binary decimals of the fraction. The sign is stored as a single bit which is 0 for positive and 1
for negative numbers. The exponent is stored as a biased binary integer, and the fraction is
stored as the binary digits. The exponent is always normalized, if possible, so that the value
before the decimal point is 1. This '1' is not included in the representation, except in the
long double format. The formats can be expressed as follows:
</p>

<div class="org-src-container">

<pre class="src src-C++"><span class="org-keyword">struct</span> <span class="org-type">Sfloat</span> {
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">fraction</span> : 23; <span class="org-comment-delimiter">// </span><span class="org-comment">fractional part</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">exponent</span> : 8; <span class="org-comment-delimiter">// </span><span class="org-comment">exponent + 0x7F</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">sign</span> : 1; <span class="org-comment-delimiter">// </span><span class="org-comment">sign bit</span>
};
<span class="org-keyword">struct</span> <span class="org-type">Sdouble</span> {
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">fraction</span> : 52; <span class="org-comment-delimiter">// </span><span class="org-comment">fractional part</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">exponent</span> : 11; <span class="org-comment-delimiter">// </span><span class="org-comment">exponent + 0x3FF</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">sign</span> : 1; <span class="org-comment-delimiter">// </span><span class="org-comment">sign bit</span>
};
<span class="org-keyword">struct</span> <span class="org-type">Slongdouble</span> {
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">fraction</span> : 63; <span class="org-comment-delimiter">// </span><span class="org-comment">fractional part</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">one</span> : 1; <span class="org-comment-delimiter">// </span><span class="org-comment">always 1 if nonzero and normal</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">exponent</span> : 15; <span class="org-comment-delimiter">// </span><span class="org-comment">exponent + 0x3FFF</span>
  <span class="org-type">unsigned</span> <span class="org-type">int</span> <span class="org-variable-name">sign</span> : 1; <span class="org-comment-delimiter">// </span><span class="org-comment">sign bit</span>
};
</pre>
</div>

<p>
The values of nonzero floating point numbers can be calculated as follows:
</p>
<pre class="example">
floatvalue = (-1)^sign ⋅ 2^(exponent-127) ⋅ (1 + fraction ⋅ 2^-23).
doublevalue = (-1)^sign ⋅ 2^(exponent-1023) ⋅ (1 + fraction ⋅ 2^-52).
longdoublevalue = (-1)^ sign ⋅ 2^(exponent-16383) ⋅ (one + fraction ⋅ 2^-63).
</pre>
<p>
The value is zero if all bits except the sign bit are zero. Zero can be represented with or
without the sign bit.
</p>

<p>
The fact that the floating point format is standardized allows us to manipulate the different
parts of the floating point representation directly with the use of integer operations. This can
be an advantage because integer operations are faster than floating point operations.
</p>

<p>
In general, it is faster to access a floating point variable as an integer if it is stored in
memory, but not if it is a register variable. The union forces the variable to be stored in
memory, at least temporarily. Using the methods in the above examples will therefore be a
disadvantage if other nearby parts of the code could benefit from using registers for the
same variables.（变量只能够存放在内存上而不能够在寄存器中）
</p>

<p>
It is not recommended to modify a double by modifying only half of it, for example if you
want to flip the sign bit in the above example with u.i(1) ^= 0x80000000; because this
is likely to generate a store forwarding delay in the CPU (See manual 3: "The
microarchitecture of Intel, AMD and VIA CPUs"). This can be avoided in 64-bit systems by
using a 64-bit integer rather than two 32-bit integers to alias upon the double.
</p>

<p>
Another problem with accessing 32 bits of a 64-bit double is that it is not portable to systems
with big-endian storage. Example 14.23b and 14.30 will therefore need modification if
implemented on other platforms with big-endian storage. All x86 platforms (Windows, Linux,
BSD, Intel-based Mac OS, etc.) have little-endian storage, but other systems may have big
endian storage (e.g. PowerPC).
</p>

<p>
#note: 比较保险的做法应该是只读取这些变量而不改写，并且需要针对大小端做判断
</p>
</div>
</div>

<div id="outline-container-sec-14-10" class="outline-3">
<h3 id="sec-14-10"><span class="section-number-3">14.10</span> Mathematical functions</h3>
<div class="outline-text-3" id="text-14-10">
<p>
The most common mathematical functions such as logarithms, exponential functions,
trigonometric functions, etc. are implemented in hardware in the x86 CPUs. However, a
software implementation is faster than the hardware implementation in most cases when the
SSE2 instruction set is available. The best compilers use the software implementation if the
SSE2 instruction set is enabled.（尽可能使用软件实现的数学函数）
</p>

<p>
The advantage of using a software implementation rather than a hardware implementation
of these functions is higher for single precision than for double precision. But the software
implementation is faster than the hardware implementation in most cases, even for double
precision.（软件实现较硬件实现而言，在单精度浮点上优化更多。但是在大部分情况下即使针对双精度软件实现效果也更好）
</p>
</div>
</div>

<div id="outline-container-sec-14-11" class="outline-3">
<h3 id="sec-14-11"><span class="section-number-3">14.11</span> Static versus dynamic libraries</h3>
<div class="outline-text-3" id="text-14-11">
<p>
The advantages of using static linking rather than dynamic linking are:
</p>
<ul class="org-ul">
<li>Static linking includes only the part of the library that is actually needed by the application, while dynamic linking makes the entire library (or at least a large part of it) load into memory even when just a single function from the library is needed.
</li>
<li>All the code is included in a single executable file when static linking is used. Dynamic linking makes it necessary to load several files when the program is started.
</li>
<li>It takes longer time to call a function in a dynamic library than in a static link library because it needs an extra jump through a pointer in an import table and possibly also a lookup in a procedure linkage table (PLT).（减少跳转次数）
</li>
<li>The memory space becomes more fragmented when the code is distributed between multiple dynamic libraries. The dynamic libraries are loaded at round memory addresses divisible by the memory page size (4096). This will make all dynamic libraries contend for the same cache lines. This makes code caching and data caching less efficient.
</li>
<li>Dynamic libraries are less efficient in some systems because of the needs of position-independent code, see below.（pic代码效率）
</li>
<li>Installing a second application that uses a newer version of the same dynamic library can change the behavior of the first application if dynamic linking is used, but not if static linking is used.
</li>
</ul>

<p>
The advantages of dynamic linking are:
</p>
<ul class="org-ul">
<li>Multiple applications running simultaneously can share the same dynamic libraries without the need to load more than one instance of the library into memory. This is useful on servers that run many processes simultaneously. Actually, only the code section and read-only data sections can be shared. Any writable data section needs one instance for each process.
</li>
<li>A dynamic library can be updated to a new version without the need to update the program that calls it.
</li>
<li>A dynamic library can be called from programming languages that do not support static linking.
</li>
<li>A dynamic library can be useful for making plug-ins that add functionality to an existing program.
</li>
</ul>

<p>
The memory address at which a dynamic library is loaded cannot be determined in
advance, because a fixed address might clash with another dynamic library requiring the
same address. There are two commonly used methods for dealing with this problem:
</p>
<ul class="org-ul">
<li>Relocation. All pointers and addresses in the code are modified, if necessary, to fit the actual load address. Relocation is done by the linker and the loader.
</li>
<li>Position-independent code. All addresses in the code are relative to the current position.
</li>
</ul>

<p>
Windows DLLs use relocation. The DLLs are relocated by the linker to a specific load
address. If this address is not vacant then the DLL is relocated (rebased) once more by the
loader to a different address. A call from the main executable to a function in a DLL goes
through an import table or a pointer. A variable in a DLL can be accessed from main
through an imported pointer, but this feature is seldom used. It is more common to
exchange data or pointers to data through function calls. Internal references to data within
the DLL use absolute references in 32 bit mode and mostly relative references in 64 bit
mode. The latter is slightly more efficient because relative references do not need relocation
at load time.
</p>
</div>
</div>

<div id="outline-container-sec-14-12" class="outline-3">
<h3 id="sec-14-12"><span class="section-number-3">14.12</span> Position-independent code</h3>
<div class="outline-text-3" id="text-14-12">
<p>
#note: 之前写过一篇有关于pic的<a href="pic.html">文章</a>
</p>

<p>
A code that is compiled as position-independent has the following features:
</p>
<ul class="org-ul">
<li>The code section contains no absolute addresses that need relocation, but only self-relative addresses. Therefore, the code section can be loaded at an arbitrary memory address and shared between multiple processes.（代码区域没有使用需要重定位的绝对地址，而使用自身相对地址，所以可以被载入到内存的任意位置）
</li>
<li>The data section is not shared between multiple processes because it often contains writeable data. Therefore, the data section may contain pointers or addresses that need relocation.（数据区域内有指针和地址需要重定位，因为数据区域并不是只读的可能存在多份）
</li>
<li>All public functions and public data can be overridden in Linux and BSD. If a function in the main executable has the same name as a function in a shared object, then the version in main will take precedence, not only when called from main, but also when called from the shared object. Likewise, when a global variable in main has the same name as a global variable in the shared object, then the instance in main will be used, even when accessed from the shared object. This so-called symbol interposition is intended to mimic the behavior of static libraries.
</li>
<li>A shared object has a table of pointers to its functions, called procedure linkage table (PLT) and a table of pointers to its variables called global offset table (GOT) in order to implement this "override" feature. All accesses to functions and public variables go through the PLT and GOT.（PLT来存储函数指针，使用GOT来存储变量指针）
</li>
</ul>

<p>
The symbol interposition feature that allows overriding of public functions and data in Linux
and BSD comes at a high price, and in most libraries it is never used. Whenever a function
in a shared object is called, it is necessary to look up the function address in the procedure
linkage table (PLT). And whenever a public variable in a shared object is accessed, it is
necessary to first look up the address of the variable in the global offset table (GOT). These
table lookups are needed even when the function or variable is accessed from within the
same shared object. Obviously, all these table lookup operations slow down the execution
considerably.（不管是查找函数还是变量代价都是非常高的）
</p>

<p>
Another serious burden is the calculation of self-relative references in 32-bit mode. The 32-
bit x86 instruction set has no instruction for self-relative addressing of data. The code goes
through the following steps to access a public data object: (1) get its own address through a
function call. (2) find the GOT through a self-relative address. (3) look up the address of the
data object in the GOT, and finally (4) access the data object through this address. Step (1)
is not needed in 64-bit mode because the x86-64 instruction set supports self-relative
addressing.（看上去查找数据的开销远高于查找函数的开销）
</p>

<p>
It is possible to compile a shared object without the -fpic option. Then we get rid of all
the problems mentioned above. Now the code will run faster because we can access
internal variables and internal functions in a single step rather than the complicated address
calculation and table lookup mechanisms explained above. A shared object compiled
without -fpic is much faster, except perhaps for a very large shared object where most of
the functions are never called. The disadvantage of compiling without -fpic in 32-bit Linux
is that the loader will have more references to relocate, but these address calculations are
done only once, while the runtime address calculations have to be done at every access.
The code section needs one instance for each process when compiled without -fpic
because the relocations in the code section will be different for each process. Obviously, we
loose the ability to override public symbols, but this feature is rarely needed anyway.
（对于32位机器可以关闭-fpic选项，那么这样便没有pic代码。所有的重定位都在链接阶段完成，并且需要和使用的process进行联编）
</p>

<p>
The procedure to calculate self-relative addresses is much simpler in 64-bit mode because
the 64-bit instruction set has support for relative addressing of data. The need for special
position-independent code is smaller because relative addresses are often used by default
anyway in 64-bit code. However, we still want to get rid of the GOT and PLT lookups for
local references.（对于64位系统来说为自身相对地址提供了支持，但是依然需要解决查找GOT和PLT的问题）
</p>

<p>
If we compile the shared object without -fpic in 64 bit mode, we encounter another
problem. The compiler sometimes uses 32-bit absolute addresses. This works in the main
executable because it is sure to be loaded at an address below 2 GB, but not in a shared
object which is typically loaded at a higher address which can't be reached with a 32-bit
(signed) address. The linker will generate an error message in this case. The best solution
is to compile with the option -fpie instead of -fpic. This will generate relative
addresses in the code section, but it will not use GOT and PLT for internal references.
Therefore, it will run faster than when compiled with -fpic and it will not have the
disadvantages mentioned above for the 32-bit case. The -fpie option is less useful in 32-
bit mode, where it still uses a GOT.（在64位系统下使用-fpic会存在问题。使用-fpie可以只针对代码区域
使用自身定位地址，但是不会使用GOT/PLT来解决内部引用问题）
</p>

<p>
You can't have public variables in a 64-bit shared object made with option -fpie because
the linker makes an error message when it sees a relative reference to a public variable
where it expects a GOT entry. You can avoid this error by avoiding any public variables. All
global variables (i.e. variables defined outside any function) should be hidden by using the
declaration "static" or "__attribute__((visibility ("hidden")))". A more
complicated solution is to use inline assembly code to give the variable two names, one
global and one local, and use the local name for local references.（因为-fpie不会使用GOT，所以便
没有办法解决全局变量问题，所有变量必须只对内可见）
</p>
</div>
</div>

<div id="outline-container-sec-14-13" class="outline-3">
<h3 id="sec-14-13"><span class="section-number-3">14.13</span> System programming</h3>
</div>
</div>
<div id="outline-container-sec-15" class="outline-2">
<h2 id="sec-15"><span class="section-number-2">15</span> Metaprogramming</h2>
</div>
<div id="outline-container-sec-16" class="outline-2">
<h2 id="sec-16"><span class="section-number-2">16</span> Testing speed</h2>
<div class="outline-text-2" id="text-16">
<p>
The measured time is interpreted in the following way. The first count is always higher than
the subsequent counts. This is the time it takes to execute CriticalFunction when code
and data are not cached. The subsequent counts give the execution time when code and
data are cached as good as possible. The first count and the subsequent counts represent
the "worst case" and "best case" values. Which of these two values is closest to the truth
depends on whether CriticalFunction is called once or multiple times in the final
program and whether there is other code that uses the cache in between the calls to
CriticalFunction. If your optimization effort is concentrated on CPU efficiency then it is
the "best case" counts that you should look at to see if a certain modification is profitable.
On the other hand, if your optimization effort is concentrated on arranging data in order to
improve cache efficiency, then you may also look at the "worst case" counts.（如果关注CPU效率那么就看
best case也就是非初次情况下时钟耗费，而如果关注cache效率那么就看worst case也就是初次启动情况下时钟耗费）
</p>

<p>
Occasionally, the clock counts that you measure are much higher than normal. This
happens when a task switch occurs during execution of CriticalFunction. You cannot
avoid this in a protected operating system, but you can reduce the problem by increasing
the thread priority before the test and setting the priority back to normal afterwards.
（通过给予线程最高优先级别来减少任务切换带来的影响）
</p>

<p>
The time stamp counter is a little inaccurate on microprocessors that can change the clock
frequency (Intel SpeedStep® technology). A more accurate measurement can be obtained
with a performance monitor counter for "core clock cycles", using the test program
mentioned above.（如果CPU频率会自动调节的话，那么读取clock counter这种方式来计时就会存在问题）
</p>
</div>

<div id="outline-container-sec-16-1" class="outline-3">
<h3 id="sec-16-1"><span class="section-number-3">16.1</span> The pitfalls of unit-testing</h3>
</div>
<div id="outline-container-sec-16-2" class="outline-3">
<h3 id="sec-16-2"><span class="section-number-3">16.2</span> Worst-case testing</h3>
<div class="outline-text-3" id="text-16-2">
<p>
Each of the following methods could possibly be relevant when testing worst-case
performance:
</p>
<ul class="org-ul">
<li>The first time you activate a particular part of the program, it is likely to be slower than the subsequent times because of lazy loading of the code, cache misses and branch mispredictions.
</li>
<li>Test the whole software package, including all runtime libraries and frameworks, rather than isolating a single function. Switch between different parts of the software package in order to increase the likelihood that certain parts of the program code are uncached or even swapped to disk.
</li>
<li>Software that relies on network resources and servers should be tested on a network with heavy traffic and a server in full use rather than a dedicated test server.
</li>
<li>Use large data files and databases with lots of data.
</li>
<li>Use an old computer with a slow CPU, an insufficient amount of RAM, a lot of irrelevant software installed, a lot of background processes running, and a fragmented hard disk.
</li>
<li>Test with different brands of CPUs, different types of graphics cards, etc.
</li>
<li>Use an antivirus program that scans all files on access.（减少操作系统对文件缓存影响）
</li>
<li>Run multiple processes or threads simultaneously. If the microprocessor has hyperthreading, then try to run two threads in each processor core.
</li>
<li>Try to allocate more RAM than there is, in order to force the swapping of memory to disk.
</li>
<li>Provoke cache misses by making the code size or data used in the innermost loop bigger than the cache size. Alternatively, you may actively invalidate the cache. The operating system may have a function for this purpose, or you may use the _mm_clflush intrinsic function. （通过指令强制cache失效）
</li>
<li>Provoke branch mispredictions by making the data more random than normal.（使用随机数据来触发分支误判）
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-17" class="outline-2">
<h2 id="sec-17"><span class="section-number-2">17</span> Optimization in embedded systems</h2>
</div>
<div id="outline-container-sec-18" class="outline-2">
<h2 id="sec-18"><span class="section-number-2">18</span> Overview of compiler options</h2>
</div>
</div>
</body>
</html>
